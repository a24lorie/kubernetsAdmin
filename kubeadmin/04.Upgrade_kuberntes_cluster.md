### Introduction

`kubeadm`  supports upgrading Kubernetes clusters. In this Lab Step, you will be upgrading Kubernetes from version 1.13.4 to version 1.14.1. Although upgrading is supported, you should always take care to understand any changes between releases by  [reading the release notes](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md)  and how they could impact your workloads. You should always backup important data before upgrading, and test upgrades before deploying them to production.

The upgrade process follows the general procedure of:

1.  Upgrading the Kubernetes control plane with  `kubeadm`  (Kubernetes components and add-ons excluding the CNI)
2.  Manually upgrading the CNI network plugin, if applicable (For this Lab, the installed version 3.1 of Calico is already the appropriate one for Kubernetes version 1.11.1)
3.  Upgrading the Kubernetes packages (`kubelet`,  `kubeadm`,  `kubectl`) on the master and worker nodes
4.  Upgrading the  `kubelet`  config on worker nodes with  `kubeadm`

### Instructions

1. Download version 1.14.1 of  `kubeadm`:
```
# Update the kubeadm binary with version 1.14.1  
sudo curl -sSL https://dl.k8s.io/release/v1.14.1/bin/linux/amd64/kubeadm -o /usr/bin/kubeadm
```
Currently, you cannot upgrade  `kubeadm`  using the apt package manager until after upgrading the control plane. This limitation should be removed in future versions of  `kubeadm`.

2. Generate an upgrade plan for upgrading Kubernetes to version 1.14.1:
```
sudo kubeadm upgrade plan v1.14.1
```
![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid0-dbbb067c-ff56-4554-b1f6-8258951e77e7.png)

As the output explains, several checks are performed, and the requirements for upgrading the cluster are first verified. A reminder that you need to manually upgrade the kubelet on each node in the cluster is then displayed. Future versions may remove this manual step. Finally, a summary of the planned version changes for all the cluster components (**COMPONENT**) is presented.

3. Apply the upgrade plan by issuing the following command and entering  _y_  when prompted:
```
sudo kubeadm upgrade apply v1.14.1
```
`kubeadm`  begins upgrading the cluster components on the master node. Read through the output to understand what steps are being performed. It takes approximately four minutes to complete. You will see the following success message to know everything went as expected:

![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid1-f0f83b53-73fb-4777-8eb9-ecb47791bf10.png)

_Note_: If the upgrade procedure times out, you can safely try again until it succeeds. The upgrade command is idempotent so you can run the command as many times as required to complete the upgrade. This issue is being worked on and should be resolved in future versions of  `kubeadm`.

4. Prepare to upgrade the master node's kubelet by draining the node:
```
kubectl drain $HOSTNAME --ignore-daemonsets
```
![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid2-90503fcf-967c-43e9-bdf3-d4f28a7efa51.png)

5. Upgrade the  `kubelet`,  `kubeadm`, and  `kubectl`  apt packages:
```
sudo apt-get update
sudo apt-get upgrade -y --allow-change-held-packages \  
     kubelet=1.14.1-00 kubeadm=1.14.1-00 kubectl=1.14.1-00 kubernetes-cni=0.7.5-00
```
The upgrade takes a minute to complete.

_Note_: If you see a **Configuring grub-pc**  menu, select **Keep the local version currently installed**:

![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid2-f493d71c-37a9-4011-9982-6a9854ca05fd.png)

Press _space_  to select the first device, followed by _enter_  to press the **<Ok>**  button:

![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid4-0832d3e5-5fcd-4bf7-a99a-5d3f1cb4017a.png)

6. Uncordon the master to allow pods to be scheduled on it now that is has been upgraded:
```
kubectl uncordon $HOSTNAME
```
7. Get the node information to confirm that the version of the master is 1.14.1:
```
kubectl get nodes
```
![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid3-e0dba1f4-827c-4b90-bc28-4e5651854343.png)

8. Drain the worker node to prepare it for upgrading:
```
# Get the worker's name  
worker_name=$(kubectl get nodes | grep \<none\> | cut -d' ' -f1)  
# Drain the worker node  
kubectl drain $worker_name --ignore-daemonsets
```
![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid0-a506e41f-541c-4d39-9fe4-330789d24139.png)

9. In the SSH shell connected to the  worker node, drain the node and upgrade the Kubernetes packages:
```
sudo apt-get update  
sudo apt-get upgrade -y --allow-change-held-packages \  
     kubelet=1.14.1-00 kubeadm=1.14.1-00 kubectl=1.14.1-00 kubernetes-cni=0.7.5-00
```
10. Upgrade the worker node's  `kubelet`  config using  `kubeadm`:
```
sudo kubeadm upgrade node config --kubelet-version v1.14.1
```
![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid5-c03cb993-6132-4d9e-b32f-fb019aecb5b5.png)

11. Restart the worker node's  `kubelet`:
```
sudo systemctl restart kubelet
```
12. Return to the master's SSH shell and uncordon the worker node:
```
kubectl uncordon $worker_name
```
![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid5-1dcb1216-c3f7-4186-9291-c78678d9afd3.png)

13. Confirm the worker node is ready and running version 1.14.1:
```
kubectl get nodes
```
![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid7-f3e404c4-6757-4503-aa16-66f6be397fda.png)

The upgrade process is now complete. You can create some pods if you want to further test the upgrade succeeded.

### Summary

In this Lab Step, you went through the process of upgrading the cluster using  `kubeadm`.

This brings you to the end of the guided portion of the Lab. Before ending, there are a few points to make regarding considerations for production clusters:

-   All of the instances in the Lab were in a public subnet. This was to allow to focus more on creating the cluster and not AWS-specific network security. In practice, you would use a private subnet for the cluster nodes, and use a load balancer in a public subnet to access the API server and a bastion host in a public subnet to connect to the instances via SSH.
-   All of the instances shared the same instance role and have the same permissions. In practice, the worker nodes do not need as many privileges as a master node. The master needs to be able to create load balancers for load balanced services and volumes for persistent volumes. Worker nodes do not. As a best practice, using the least amount of privilege possible for each type of node will improve security of your cluster.
-   In cloud environments, you can use auto scaling capabilities to automatically add worker nodes to your cluster when certain conditions are met, such as all nodes using a lot of CPU.
-   You would want a highly-available control plane in production. At least you would want to have monitoring in place to automatically restart the master if it fails, but running a multi-master cluster will prevent downtime and protect you from zone outages.

**Challenge** (Optional)

If you have time remaining in your Lab session, try to add the third EC2 instance, **instance-c**, to the cluster. Refer back to previous Lab Steps, as necessary. Keep in mind that the cluster is now running version 1.14.1, but instance-c still has earlier versions of all the Kubernetes packages installed. Use  `kubectl get nodes`  to check the nodes in the cluster and their versions. Use  `kubectl describe nodes`  to debug any issues you might encounter joining instance-c to the cluster.
