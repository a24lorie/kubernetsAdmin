### Introduction

Kubernetes is made up of several core components. Recall that every node in a Kubernetes cluster has the following components:

-   **kubelet**: The primary node agent that accepts pod specifications
-   **Container runtime**: Software responsible for running containers, for example, Docker
-   **kube-proxy**: Implements network rules and connection forwarding to enable the Kubernetes service abstraction

In addition to the components on every node, master nodes also include the following components to provide the Kubernetes control plane:

-   **kube-apiserver**: Exposes the Kubernetes API
-   **etcd**: Key-value store for backing cluster data
-   **kube-scheduler**: Responsible for scheduling pods onto nodes
-   **kube-controller-manager**: Responsible for running Kubernetes controllers, for example, the node controller that responds to changes in a node's status

In previous Lab Steps, you have seen how to troubleshoot the kubelet and could use similar techniques for the container runtime. This Lab Step focuses on the remaining components. You will understand how the components are deployed in a cluster, and how to diagnose and resolve issues with the components.

### Instructions

1. List all of the pods in the `kube-system`  namespace:
```
kubectl get pods -n kube-system
```
![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid0-ecd7cddf-fc8c-463c-8aae-4457d0013709.png)

You can see that all of the components besides the kubelet and the container runtime have pods running in the cluster's `kube-system`  namespace, which is reserved for resources created by the Kubernetes system. Namely, **etcd-**..., **kube-apiserver-**..., **kube-controller-manager-**..., **kube-proxy-**..., and **kube-scheduler**. This means that troubleshooting issues with these components will mainly involve working within Kubernetes. You will see how each of the components is deployed within the cluster in following instructions.

It is worth pausing to explain the other pods that are included in the list. The **calico-**... pods are related to the cluster networking implementation. Calico is one of many networking options that implement the container network interface (CNI). **kube-dns-**... is one implementation of a cluster DNS for Kubernetes. The **kubernetes-dashboard****-**  is a graphical interface for managing the Kubernetes cluster. Because these are all deployed using pods in Kubernetes, you could troubleshoot them using the same general troubleshooting techniques that you will use in this and the following Lab Step

2. List all of the daemonsets in the  `kube-system`  namespace:
```
kubectl get daemonset -n kube-system
```
![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid1-436ece92-5405-406c-9dd4-a829b5f4619e.png)

Observe that there is a **kube-proxy** daemonset. Daemonsets are commonly used to ensure that a copy of a pod is run on every node in a cluster. Because every node in the cluster needs to run a kube-proxy, a daemonset is the natural way to deploy it in Kubernetes. The **DESIRED**  count should equal the **READY** count under normal operation. If it does not, you could analyze the output of  `kubectl get pods -n kube-system -o wide`  to see which kube-proxy pod is not ready and which node it is scheduled on.

3. View the kube-proxy daemonset resource to understand how it is configured:
```
kubectl get daemonset kube-proxy -n kube-system -o yaml --export | more
```
The  `get`  command with  `-o yaml --export` options is a useful pattern to use for generating resource specification YAML files from existing resources. The  `--export`  option strips out any cluster-specific information and makes the output more portable. The daemonset specification is quite complex in this case making use of  **initContainers**,  **configMap**, and  **serviceAccount**. Because the specification is generated by the system, it is not likely to need troubleshooting. However, it is useful to understand everything that is required for the kube-proxy to function.

4. View the logs of one of the kube-proxy pods:
```
# proxy_pod_1 is a variable with the name of the first kube-proxy pod  
proxy_pod_1=$(kubectl get pods -n kube-system | grep proxy | cut -d" " -f1 | head -1)  
kubectl logs -n kube-system $proxy_pod_1
```
The  `logs`  command is the first place to check when you detect something has gone wrong with one of the kube-system pods. In this case, there are no errors and everything is operating normally.

5. Delete the kube-proxy pod and immediately get the daemonsets after:
```
kubectl delete pod $proxy_pod_1 -n kube-system  
kubectl get daemonset -n kube-system 
```
![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid0-bd2519ef-c07e-4700-b028-3f002dff0ed0.png)

While the pod is gracefully being deleted, the **READY**  count drops down to  **3**. However, because kube-proxy is deployed using a daemonset, there will be 4 ready pods within 30 seconds of the deletion. This demonstrates the self-healing capabilities of Kubernetes components deployed in Kubernetes.

6. List the system daemonsets to confirm that a new kube-proxy pod is automatically created to replace the deleted one:
```
kubectl get daemonset -n kube-system
```
7. Connect to the master node using SSH:
```
ssh $master_ip
```
The remaining components are deployed on the master. The way that they are created is different from the way you usually create pods as you will see in the following instructions.

8. Attempt to change the image that the kube-apiserver pod is using:
```
# Get the name of the kube-apiserver pod  
apiserver_pod=$(kubectl get pods -n kube-system | grep apiserver | cut -d" " -f1)  
# change the pod's image to hello-world using the patch command  
kubectl patch pod $apiserver_pod -n kube-system \  
  -p '{"spec":{"containers":[{"name":"kube-apiserver","image":"hello-world"}]}}'
```
![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid1-2cc60ff0-b65a-411f-a571-c15d949cfae2.png)

The success message leads you to believe the kube-apiserver is now running the hello-world image. If this is true, kubectl will no longer work because it depends on the kube-apiserver image to be running.

9. Describe the kube-apiserver pod:
```
kubectl describe pod $apiserver_pod -n kube-system | more
```
The first thing you should notice is that the command succeeds. This means that the kube-apiserver container is not using the hello-world image. In the **Containers**  section of the output you see:

![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid2-c62d6ccb-5700-4074-8c6a-abeb438f9140.png)

The **Image** is reporting **hello-world**, but the **Image ID** is still the ID of the correct kube-apiserver image (**k8s.gcr.io/kube-apiserver-amd64**...). There are also no **Events**  listed for the container, which you would expect to see if the image was changed. So what exactly happened? The pod returned by  `kubectl`, which is what the API server returns, is actually a "mirror pod" of the real pod running on the master node. The changes you make to the mirror pod are not reflected by the actual underlying pod. For example, you can delete the pod entirely using  `kubectl`, but the real pod is not affected and the API server will continue to function.

Pods that behave in this way are called  [static pods](https://kubernetes.io/docs/tasks/administer-cluster/static-pod/). Static pods are managed directly by the node's kubelet and not by the API server. Static pods are configured by placing pod specifications in a manifest directory that the kubelet periodically reads to keep the pods in sync with the specifications on disk.

10. Get the status of the kubelet and find the  `pod-manifest-path`:
```
systemctl status kubelet
```
![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid4-25e2e7b9-6fac-4fc1-9531-75f2b7c58b45.png)

The  `--pod-manifest-path`  option of the kubelet sets the directory of the static pod specifications, in this case, **/etc/kubernetes/manifests**.

As a side note, you may also see the most recent message at the bottom of the output stating that the  **kube-apiserver** **mirror pod** was deleted because it is out of date:

![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid3-aecd7dea-4c85-49b0-a04d-98b1c2e70543.png)

11. List the contents of the /etc/kubernetes/manifests directory:
```
ls /etc/kubernetes/manifests
```
![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid5-bb079d3e-ab9c-4a87-b3d7-bce9100f0a17.png)

There is one pod specification for each of the master components. The specifications are the same as a normal pod specification. The difference with static pods is only how the pods are managed directly by the kubelet insead of the API server. The kubelet creates mirror pods of the static pods in the API server so that they are visible using the API server and  `kubectl`, but they cannot modify static pods.

12. View the contents of the etcd pod specification:
```
sudo more /etc/kubernetes/manifests/etcd.yaml
```
Because the file is autogenerated and not likely to be edited, there should not be issues with the specification. However, it is instructive to review because it illustrates a few points about working with etcd. To  [backup or restore a cluster](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster)  you need to work with etcd. In the context of troubleshooting, you have the option of restoring the cluster to the state stored in a backup when it is difficult to repair a cluster ,but you have a working backup. Focus now on the the **spec**  section of the output:

![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid0-5c179884-fc81-421c-9357-15f303c9daf6.png)

In the **command** you can see several useful pieces of information:

-   The URL that etcd is listening on (**https://127.0.0.1:2379**)
-   The variety of SSL/TLS certificates that are used and where they are located, for example the **peer-**... options that all point to files in **/etc/kubernetes/pki/etcd/**

Further down is the  **livenessProbe**, which gives an example command for  **etcdctl**, the command-line client for etcd. The command simply  **get**s the value of a key named **foo**. Notice that the command also specifies options for the etcd endpoint (**--endpoints**), the certificate authority certificate (**--cacert**), and a key (**--key**) and certificate (**--cert**) for client authentication. The etcd database is using version 3, so the environment variable  **ETCDCTL_API**  must be set to  **3**. Further down still, you can see that the container uses the host's network (**hostNetwork: true**). This means you can use etcdctl installed on the host to interact with etcd and not rely on containers within the container's network.

You can also see the **Command** from the output of  `kubectl describe`  when that is more convenient, thanks to the mirror pod.

13. Display the TCP ports that are being listened to on the host network to confirm the etcd endpoint (127.0.0.1:2379) is being listened to:
```
ss -tl
```
![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid1-dd081d57-71aa-46c0-80b1-ec9ccb9d88b2.png)

The endpoint appears fifth from the top in the image above.

14. Run an etcdctl command in the etcd pod's container by using  `kubectl exec`:
```
etcd_pod=$(kubectl get pods -n kube-system | grep ^etcd | cut -d" " -f1)  
kubectl exec -n kube-system $etcd_pod -- \  
  /bin/sh -ec \  
  'ETCDCTL_API=3 \  
   etcdctl \  
   --endpoints=127.0.0.1:2379 \  
   --cacert=/etc/kubernetes/pki/etcd/ca.crt \  
   --cert=/etc/kubernetes/pki/etcd/peer.crt \  
   --key=/etc/kubernetes/pki/etcd/peer.key \  
   get \  
   /registry/clusterrolebindings/cluster-admin'
```
![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid0-0cc3e5fd-f1e7-4419-a471-796ab89be2c4.png)

The  `kubectl exec`  command allows you to run commands in containers. The command to run comes after the  `--`  symbol and is based on the example liveness probe command from the etcd pod specification. The peer key and certificate are used, and the `registry/`clusterrolebindings`/cluster-admin`  key is retrieved. You should never have to retrieve individual keys directly from etcd, but the example shows you how to use etcdctl and confirms that Kubernetes state is stored in etcd.

15. Disconnect from the master node:
```
exit
```
16. Enter the following command to get an overview of the health of the master's components:
```
kubectl get componentstatus
```
![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid1-c9e3e7b4-76b8-4211-9cce-f3018cebe450.png)

This command can quickly identify any issues in the listed components. The API server is not listed since  `kubectl`  would not be functional if the API server wasn not healthy. If a component is not healthy, you can use the methods discussed before to narrow in on the cause and attempt to remedy the situation.

17. Display the help for the cluster-info command:
```
kubectl cluster-info --help
```
![alt](https://assets.cloudacademy.com/bakery/media/uploads/blobid2-60279a22-bd50-4253-b522-a45db1dbf843.png)

The  `cluster-info`  command is useful if you need to know the address of the master or other cluster services. You can also use the **dump**  command to easily get a concatenation of resource specifications, logs, and other useful diagnostic information. Redirecting the output to a file and searching the contents is a useful way to work with the massive dump.

### Summary

In this Lab Step, you learned how to troubleshoot Kubernetes components and also learned about daemonsets, static pods, how to run commands in containers, and how to work with etcd from the command line.
